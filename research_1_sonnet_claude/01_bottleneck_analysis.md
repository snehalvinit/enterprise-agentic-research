# 01 â€” Bottleneck Analysis: Smart-Segmentation System

> **Research ID:** research_1_sonnet_claude
> **System Analyzed:** Smart-Segmentation (Agentic Framework)
> **Analysis Date:** February 2026
> **Severity Scale:** ğŸ”´ Critical | ğŸŸ  High | ğŸŸ¡ Medium | ğŸŸ¢ Low

---

## Executive Summary

Smart-Segmentation is a sophisticated multi-agent customer segmentation platform built on Google's ADK framework, using Azure GPT-4o as its LLM backend, Milvus for vector search, and PostgreSQL for session persistence. The system successfully handles complex natural language queries about customer segments and translates them into structured database queries.

However, after deep analysis of the codebase, **17 critical bottlenecks** were identified across 6 dimensions: architecture, prompt design, evaluation, scalability, reliability, and missing capabilities. These issues collectively limit the system's ability to scale, self-improve, maintain quality under load, and operate as a true enterprise-grade platform.

This document catalogs every bottleneck in detail, with root cause analysis, concrete examples from the code, and the impact on system behavior.

---

## Table of Contents

1. [Architecture Bottlenecks](#1-architecture-bottlenecks)
2. [Prompt Design Flaws](#2-prompt-design-flaws)
3. [Evaluation Gaps](#3-evaluation-gaps)
4. [Scalability Limits](#4-scalability-limits)
5. [Reliability Problems](#5-reliability-problems)
6. [Missing Capabilities](#6-missing-capabilities)
7. [Severity Summary Table](#7-severity-summary-table)
8. [Root Cause Map](#8-root-cause-map)

---

## 1. Architecture Bottlenecks

### 1.1 ğŸ”´ Strictly Sequential Agent Pipeline (No Parallelization)

**Where in code:** `sub_agents/new_segment_creation/agent.py` â€” 4-step sequential agent chain

**The Problem:**
The NSC (New Segment Creation) pipeline runs 4 steps in strict sequence:
1. `segment_logic_decomposer_agent` â†’ must finish first
2. `segment_date_tagger_agent` â†’ runs after #1
3. `facet_value_operator_mapper_agent` â†’ runs after #2
4. `segment_format_generator` â†’ runs after #3

In practice, steps 2 and 3 are **not always dependent** on each other. The date tagger and facet mapper could run in parallel once decomposition is done, but the architecture doesn't allow it. For complex segments with 4-5 sub-segments, this means:

- **Actual latency:** 12-20 seconds per query
- **Possible with parallelization:** 6-10 seconds per query
- **Impact:** Poor UX, users abandon long-running queries

```python
# Current: Sequential chain (agent.py ~line 180)
result1 = decomposer_agent.run(query)
result2 = date_tagger.run(result1)  # Could be parallel with result3
result3 = facet_mapper.run(result2) # Each waits for full previous step
result4 = formatter.run(result3)
```

**Why it matters:** Every second of additional latency directly impacts user adoption. Enterprise users expect sub-5-second responses for complex queries.

---

### 1.2 ğŸ”´ Fragile 40+ Variable State Machine

**Where in code:** `state.py` â€” 40+ state variable definitions

**The Problem:**
The system maintains over 40 global state variables (e.g., `SegmentCreatedFlag`, `current_sub_segments`, `date_metadata`, `facet_mappings`, `user_restrictions`, etc.) across multi-turn conversations. These variables:

1. **Have no schema validation** â€” a wrong value type silently corrupts state
2. **Are not isolated per tenant** â€” global state can bleed between sessions in edge cases
3. **Have complex interdependencies** â€” `DSE` (edit agent) depends on state correctly set by `NSC` (create agent). If NSC fails mid-way, DSE gets corrupted state
4. **No state version control** â€” refactoring one variable silently breaks 3 others

**Concrete example:** If `SegmentCreatedFlag` is incorrectly `1` (due to a prior failed creation), the router sends all queries to `DirectSegmentEditorAgent` â€” which then fails because no segment actually exists.

**Why it matters:** State corruption is the #1 cause of silent incorrect behavior in multi-turn AI systems. With 40+ variables, debugging production issues is extremely difficult.

---

### â†’ Solution Architecture: Replacing the 40+ Variable State Machine

The solution is not merely to add validation to the existing flat dictionary. The flat key-string architecture is the root cause â€” it creates invisible coupling between agents, makes typing impossible at the Python level, and provides no affordance for transitions or lifecycle management. The fix requires replacing the flat dict with a **hierarchical, typed, lifecycle-aware state graph**.

Five concrete architectural patterns accomplish this, each independently valuable and compounding when combined.

---

#### Pattern 1: Pydantic Hierarchical State Models

**The Core Idea:** Replace 40+ string constants (keys into an untyped dict) with typed Pydantic models organized by domain concern. Each domain group owns its own fields, validators, and defaults.

**Before â€” Current `state.py` pattern (flat string keys, no types):**

```python
# state.py â€” 40+ bare string constants, zero validation
USER_ID = "user_id"
SESSION_ID = "session_id"
SEGMENT_FORMATTED_FLAG = "segment_formatted_flag"
FACET_CLASSIFIER_INDEX_POINTER = "facet_classifier_index_pointer"
OG_FACET_CLASSIFIER_DEPENDENCY_DICT = "og_facet_classifier_dependency_dict"
CURR_FACET_CLASSIFIER_RESOLVER_RESPONSE = "curr_facet_classifier_resolver_response"
# ... 35+ more

# Usage in agent code â€” completely untyped, any value accepted silently:
session_state[state.SEGMENT_FORMATTED_FLAG] = True   # should be int 0/1 â€” no error
session_state[state.FACET_CLASSIFIER_INDEX_POINTER] = "oops"  # should be int â€” no error
session_state[state.OG_FACET_CLASSIFIER_DEPENDENCY_DICT] = []  # should be dict â€” no error
```

**After â€” Hierarchical Pydantic models with full type safety:**

```python
# state_models.py â€” NEW: typed state organized by domain
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import Optional, Dict, List, Any
from enum import Enum
import uuid
from datetime import datetime

# â”€â”€ Domain: User & Session Identity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class UserContext(BaseModel):
    user_id: str
    user_type: str
    user_ip: Optional[str] = None
    user_agent: Optional[str] = None
    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

# â”€â”€ Domain: Pipeline Lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class PipelinePhase(str, Enum):
    IDLE = "idle"
    DECOMPOSING = "decomposing"
    DATE_TAGGING = "date_tagging"
    FACET_MAPPING = "facet_mapping"
    FORMATTING = "formatting"
    COMPLETE = "complete"
    FAILED = "failed"

# â”€â”€ Domain: Segment Decomposition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SubSegmentState(BaseModel):
    version: str
    relationship_representation: Optional[str] = None
    query_representation: Optional[str] = None
    shortlist_generation: Optional[Dict[str, Any]] = None

    @field_validator("version")
    @classmethod
    def version_must_not_be_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("sub_segment version cannot be empty string")
        return v

# â”€â”€ Domain: Facet Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class FacetClassifierState(BaseModel):
    og_dependency_dict: Optional[Dict[str, Any]] = None
    og_resolver_response: Optional[Dict[str, Any]] = None
    curr_resolver_response: Optional[Dict[str, Any]] = None
    all_index: Optional[List[int]] = None
    curr_index: Optional[int] = None
    index_pointer: int = 0

    @field_validator("index_pointer")
    @classmethod
    def pointer_must_be_non_negative(cls, v: int) -> int:
        if v < 0:
            raise ValueError(f"index_pointer must be >= 0, got {v}")
        return v

    @model_validator(mode="after")
    def pointer_within_bounds(self) -> "FacetClassifierState":
        if self.all_index and self.index_pointer >= len(self.all_index):
            raise ValueError(
                f"index_pointer {self.index_pointer} exceeds "
                f"all_index length {len(self.all_index)}"
            )
        return self

# â”€â”€ Domain: Segment Creation (NSC) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NSCState(BaseModel):
    """New Segment Creation pipeline state â€” all fields needed by the 4-step chain."""
    phase: PipelinePhase = PipelinePhase.IDLE
    sub_segment: Optional[SubSegmentState] = None
    facet_classifier: FacetClassifierState = Field(default_factory=FacetClassifierState)
    linked_facet_classifier: FacetClassifierState = Field(default_factory=FacetClassifierState)
    segment_formatted_flag: bool = False
    segmentr_representation: Optional[str] = None
    complete_query_representation: Optional[Dict[str, Any]] = None
    fvom_output: Optional[str] = None
    segment_decomposer_output: Optional[str] = None
    date_metadata: Optional[Dict[str, Any]] = None
    clarification_question: Optional[str] = None
    contextual_info: Optional[str] = None
    purchase_facet_injection: Optional[Dict[str, Any]] = None
    conversational_history: List[Dict[str, str]] = Field(default_factory=list)
    formatted_conversational_history: List[Dict[str, str]] = Field(default_factory=list)
    # Confirmation gates â€” explicit flags replacing scattered boolean ints
    decomposer_confirmed: bool = False
    fvom_confirmed: bool = False

# â”€â”€ Domain: Segment Editing (DSE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DSEState(BaseModel):
    """Direct Segment Editor pipeline state â€” only populated after NSC completes."""
    user_query: Optional[str] = None
    available_facets_data: Optional[Dict[str, Any]] = None
    sub_segment_representation: Optional[str] = None
    segmentr_representation: Optional[str] = None
    conversational_history: List[Dict[str, str]] = Field(default_factory=list)
    formatted_conversational_history: List[Dict[str, str]] = Field(default_factory=list)

# â”€â”€ Root: Full Session State (tenant-isolated root object) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SegmentationSessionState(BaseModel):
    """
    Single root state object for the entire agentic session.
    Replaces the flat dict of 40+ string-keyed values.
    Enforces: typed fields, required vs optional, cross-domain consistency.
    """
    # Identity (required at session init)
    user: UserContext
    raw_user_query: str
    caller_name: str
    agent_response: Optional[str] = None
    facet_key_identifier: Optional[str] = None
    facet_user_restrictions: Optional[List[str]] = None
    contextual_information: Optional[str] = None
    facet_operator_value_representation_version: Optional[str] = None
    facet_operator_value_representation: Optional[Dict[str, Any]] = None
    facet_value_mapper_additional_info: Optional[str] = None

    # Sub-pipeline states â€” domain-isolated, independently validated
    nsc: NSCState = Field(default_factory=NSCState)
    dse: DSEState = Field(default_factory=DSEState)

    # Routing control â€” replaces raw SegmentCreatedFlag int
    segment_created: bool = False
    segment_creation_verified: bool = False  # NSC fully completed, DSE safe to start

    # Metadata
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated_at: datetime = Field(default_factory=datetime.utcnow)
    schema_version: str = "2.0.0"  # enables migration detection

    @model_validator(mode="after")
    def dse_only_after_verified_creation(self) -> "SegmentationSessionState":
        """
        Critical invariant: DSE state must not be populated unless segment creation
        was fully verified. This prevents the SegmentCreatedFlag=1/no-actual-segment bug.
        """
        if not self.segment_creation_verified and self.dse.user_query is not None:
            raise ValueError(
                "DSEState.user_query set but segment_creation_verified=False. "
                "Cannot edit a segment that was not fully created."
            )
        return self
```

**What this buys immediately:**
- Any agent that sets `nsc.facet_classifier.index_pointer = -1` gets a `ValidationError` at write time, not a silent failure 3 turns later
- The `dse_only_after_verified_creation` validator makes the `SegmentCreatedFlag=1/no-segment-exists` bug **structurally impossible** â€” the Pydantic model refuses the state
- IDE autocomplete replaces string-key lookups â€” refactoring one field shows every breakage instantly
- `schema_version` enables migration detection when the shape changes across deployments

---

#### Pattern 2: Explicit Finite State Machine with Valid Transition Graph

**The Core Idea:** The current system uses `SegmentCreatedFlag` (an integer in a flat dict) to route between NSC and DSE. This is an implicit, undocumented state machine with no enforcement of valid transitions. Replace it with an explicit FSM where every transition is named, validated, and logged.

**Current implicit state machine (inferred from code â€” never documented):**

```
# IMPLICIT: reconstructed from routing logic in agent_routes.py
IDLE â”€â”€(user sends query)â”€â”€â†’ ROUTING
ROUTING â”€â”€(SegmentCreatedFlag == 0)â”€â”€â†’ NSC_RUNNING
ROUTING â”€â”€(SegmentCreatedFlag == 1)â”€â”€â†’ DSE_RUNNING   â† BUG: no check if segment exists
NSC_RUNNING â”€â”€(flag set mid-run)â”€â”€â†’ DSE_RUNNING      â† BUG: can skip before NSC done
# No FAILED state. No ROLLBACK. No PARTIAL_SUCCESS.
```

**After â€” Explicit FSM with transition validation:**

```python
# session_fsm.py â€” NEW: explicit state machine for session lifecycle
from enum import Enum
from typing import Set, Dict, Optional
from dataclasses import dataclass, field
import logging

logger = logging.getLogger(__name__)

class SessionPhase(str, Enum):
    IDLE = "idle"
    ROUTING = "routing"
    NSC_DECOMPOSING = "nsc_decomposing"
    NSC_DATE_TAGGING = "nsc_date_tagging"
    NSC_FACET_MAPPING = "nsc_facet_mapping"
    NSC_FORMATTING = "nsc_formatting"
    NSC_CONFIRMING = "nsc_confirming"
    NSC_COMPLETE = "nsc_complete"        # â† Only here is DSE allowed to start
    DSE_EDITING = "dse_editing"
    DSE_CONFIRMING = "dse_confirming"
    DSE_COMPLETE = "dse_complete"
    CLARIFYING = "clarifying"           # Waiting for user input
    FAILED = "failed"                   # Explicit failure state
    ROLLED_BACK = "rolled_back"         # State restored to last checkpoint

# The only valid transitions â€” any other transition raises InvalidTransitionError
VALID_TRANSITIONS: Dict[SessionPhase, Set[SessionPhase]] = {
    SessionPhase.IDLE:              {SessionPhase.ROUTING},
    SessionPhase.ROUTING:           {SessionPhase.NSC_DECOMPOSING, SessionPhase.DSE_EDITING},
    SessionPhase.NSC_DECOMPOSING:   {SessionPhase.NSC_DATE_TAGGING, SessionPhase.CLARIFYING, SessionPhase.FAILED},
    SessionPhase.NSC_DATE_TAGGING:  {SessionPhase.NSC_FACET_MAPPING, SessionPhase.CLARIFYING, SessionPhase.FAILED},
    SessionPhase.NSC_FACET_MAPPING: {SessionPhase.NSC_FORMATTING, SessionPhase.CLARIFYING, SessionPhase.FAILED},
    SessionPhase.NSC_FORMATTING:    {SessionPhase.NSC_CONFIRMING, SessionPhase.FAILED},
    SessionPhase.NSC_CONFIRMING:    {SessionPhase.NSC_COMPLETE, SessionPhase.NSC_DECOMPOSING},  # loop back on rejection
    SessionPhase.NSC_COMPLETE:      {SessionPhase.DSE_EDITING, SessionPhase.IDLE},
    SessionPhase.DSE_EDITING:       {SessionPhase.DSE_CONFIRMING, SessionPhase.CLARIFYING, SessionPhase.FAILED},
    SessionPhase.DSE_CONFIRMING:    {SessionPhase.DSE_COMPLETE, SessionPhase.DSE_EDITING},
    SessionPhase.DSE_COMPLETE:      {SessionPhase.DSE_EDITING, SessionPhase.IDLE},
    SessionPhase.CLARIFYING:        {SessionPhase.NSC_DECOMPOSING, SessionPhase.NSC_FACET_MAPPING,
                                     SessionPhase.DSE_EDITING, SessionPhase.FAILED},
    SessionPhase.FAILED:            {SessionPhase.ROLLED_BACK, SessionPhase.IDLE},
    SessionPhase.ROLLED_BACK:       {SessionPhase.IDLE, SessionPhase.NSC_DECOMPOSING},
}

class InvalidTransitionError(Exception):
    pass

class SessionFSM:
    def __init__(self, session_id: str, initial_phase: SessionPhase = SessionPhase.IDLE):
        self.session_id = session_id
        self._phase = initial_phase
        self._history: list[tuple[SessionPhase, SessionPhase, str]] = []

    @property
    def phase(self) -> SessionPhase:
        return self._phase

    def transition(self, target: SessionPhase, reason: str = "") -> None:
        allowed = VALID_TRANSITIONS.get(self._phase, set())
        if target not in allowed:
            raise InvalidTransitionError(
                f"[session={self.session_id}] Invalid transition: "
                f"{self._phase} â†’ {target}. "
                f"Allowed from {self._phase}: {allowed}. Reason attempted: '{reason}'"
            )
        logger.info(
            f"[session={self.session_id}] State transition: {self._phase} â†’ {target} | {reason}"
        )
        self._history.append((self._phase, target, reason))
        self._phase = target

    def can_enter_dse(self) -> bool:
        """Explicit guard replacing raw SegmentCreatedFlag check."""
        return self._phase in {SessionPhase.NSC_COMPLETE, SessionPhase.DSE_EDITING,
                                SessionPhase.DSE_CONFIRMING, SessionPhase.DSE_COMPLETE}

    def get_transition_history(self) -> list:
        return list(self._history)
```

**Usage in the router â€” before vs. after:**

```python
# BEFORE: implicit flag check (agent_routes.py current style)
segment_flag = session_state.get(state.SEGMENT_FORMATTED_FLAG, 0)
if segment_flag == 1:
    return direct_segment_editor_agent.run(query)  # â† no check if segment exists!
else:
    return new_segment_creation_agent.run(query)

# AFTER: explicit FSM guard
fsm = session.fsm  # attached to session object
if fsm.can_enter_dse():
    # Also verify actual segment exists in DB â€” belt and suspenders
    if not segment_repository.exists(session.user.user_id):
        raise StateInconsistencyError(
            "FSM says DSE allowed but no segment found in DB. "
            "Forcing rollback to NSC_COMPLETE."
        )
    fsm.transition(SessionPhase.DSE_EDITING, reason="user_edit_intent_detected")
    return direct_segment_editor_agent.run(query, session)
else:
    fsm.transition(SessionPhase.NSC_DECOMPOSING, reason="new_segment_intent_detected")
    return new_segment_creation_agent.run(query, session)
```

---

#### Pattern 3: Per-Session State Isolation (Eliminating Tenant Bleed)

**The Core Idea:** The current architecture stores session state in Google ADK's `session_service` keyed by `session_id`. In edge cases â€” particularly connection pool starvation or ADK session reuse bugs â€” state from one user's session can persist into another's. The fix is strict per-session container objects with explicit lifecycle.

**The root mechanism of tenant bleed in the current system:**

```python
# CURRENT: state is a flat dict attached to a session that ADK manages
# If ADK reuses a session object (pool behavior), old state keys persist
session_state[state.FACET_USER_RESTRICTIONS] = user_restrictions
# Next request from different user gets same session_state dict if pool reuses it
# user_restrictions from User A bleeds into User B's request â€” GDPR violation risk
```

**After â€” Explicit per-session state containers with factory function:**

```python
# session_container.py â€” NEW

from dataclasses import dataclass, field
from typing import Optional
from state_models import SegmentationSessionState, UserContext
from session_fsm import SessionFSM, SessionPhase
import uuid
from datetime import datetime

@dataclass
class SessionContainer:
    """
    Single, explicitly-scoped container for all state belonging to ONE session.
    Never shared. Never pooled. Created fresh per request or loaded from checkpoint.
    """
    container_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    state: SegmentationSessionState = field(default=None)
    fsm: SessionFSM = field(default=None)
    created_at: datetime = field(default_factory=datetime.utcnow)
    checkpoint_id: Optional[str] = None  # last saved checkpoint

    def __post_init__(self):
        if self.fsm is None:
            self.fsm = SessionFSM(session_id=self.container_id)

    @classmethod
    def new(cls, user: UserContext, raw_query: str, caller_name: str) -> "SessionContainer":
        """Factory: always creates a completely clean container. Never reuses."""
        container = cls()
        container.state = SegmentationSessionState(
            user=user,
            raw_user_query=raw_query,
            caller_name=caller_name,
        )
        return container

    @classmethod
    def from_checkpoint(cls, checkpoint_data: dict) -> "SessionContainer":
        """Restore container from a PostgreSQL checkpoint â€” enables resumable runs."""
        container = cls(container_id=checkpoint_data["container_id"])
        container.state = SegmentationSessionState(**checkpoint_data["state"])
        container.fsm = SessionFSM(
            session_id=container.container_id,
            initial_phase=SessionPhase(checkpoint_data["fsm_phase"])
        )
        container.checkpoint_id = checkpoint_data["checkpoint_id"]
        return container

# â”€â”€ Session Registry: tenant-isolated lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SessionRegistry:
    """
    In-memory registry (backed by Redis for multi-worker) mapping
    session_id â†’ SessionContainer. Strict isolation: no cross-session access.
    """
    def __init__(self, redis_client=None):
        self._local: dict[str, SessionContainer] = {}
        self._redis = redis_client  # optional: for multi-worker deployments

    def create(self, user: UserContext, raw_query: str, caller_name: str) -> SessionContainer:
        container = SessionContainer.new(user, raw_query, caller_name)
        self._local[container.container_id] = container
        if self._redis:
            self._redis.setex(
                f"session:{container.container_id}",
                ttl=3600,
                value=container.state.model_dump_json()
            )
        return container

    def get(self, session_id: str) -> Optional[SessionContainer]:
        if session_id in self._local:
            return self._local[session_id]
        if self._redis:
            raw = self._redis.get(f"session:{session_id}")
            if raw:
                return SessionContainer.from_checkpoint({"state": raw, "container_id": session_id})
        return None  # never returns another tenant's container

    def destroy(self, session_id: str) -> None:
        self._local.pop(session_id, None)
        if self._redis:
            self._redis.delete(f"session:{session_id}")
```

**What this buys:**
- State is scoped to `SessionContainer` â€” a plain Python object with no global mutable state
- `SessionRegistry.get(session_id)` returns `None` for unknown sessions (no accidental reuse)
- Each container is independently destroyable â€” no cleanup logic needed across agents
- Redis backend enables multi-worker (Gunicorn) deployments where containers need to cross worker boundaries

---

#### Pattern 4: Versioned State Checkpointing (LangGraph-Inspired Pattern)

**The Core Idea:** When NSC fails mid-pipeline (e.g., during FACET_MAPPING), the current system has no mechanism to resume from the last good state. All 3 successful steps are re-run. Checkpointing writes the state to PostgreSQL after each step succeeds â€” enabling resume-from-checkpoint, rollback, and audit.

**Research backing:** LangGraph (LangChain, 2024) uses thread-keyed checkpoint stores as a first-class architectural primitive. MemGPT (Berkeley, 2023) stores agent state externally so it survives process restarts. The pattern is universally adopted in production agent systems.

```python
# checkpointing.py â€” NEW: PostgreSQL-backed step-level checkpointing

from pydantic import BaseModel
from typing import Optional
import json
import uuid
from datetime import datetime

class StateCheckpoint(BaseModel):
    checkpoint_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str
    user_id: str
    fsm_phase: str                          # SessionPhase value at checkpoint time
    state_json: str                         # SegmentationSessionState.model_dump_json()
    step_name: str                          # e.g. "nsc_decompose_complete"
    created_at: datetime = Field(default_factory=datetime.utcnow)
    schema_version: str = "2.0.0"

class CheckpointRepository:
    """Writes and reads StateCheckpoints from PostgreSQL."""

    CREATE_TABLE = """
    CREATE TABLE IF NOT EXISTS session_checkpoints (
        checkpoint_id   UUID PRIMARY KEY,
        session_id      TEXT NOT NULL,
        user_id         TEXT NOT NULL,
        fsm_phase       TEXT NOT NULL,
        state_json      JSONB NOT NULL,
        step_name       TEXT NOT NULL,
        created_at      TIMESTAMPTZ DEFAULT NOW(),
        schema_version  TEXT NOT NULL
    );
    CREATE INDEX IF NOT EXISTS idx_checkpoints_session ON session_checkpoints(session_id);
    CREATE INDEX IF NOT EXISTS idx_checkpoints_user ON session_checkpoints(user_id, created_at DESC);
    """

    def __init__(self, db_pool):
        self._pool = db_pool

    async def save(self, container: "SessionContainer", step_name: str) -> StateCheckpoint:
        cp = StateCheckpoint(
            session_id=container.container_id,
            user_id=container.state.user.user_id,
            fsm_phase=container.fsm.phase.value,
            state_json=container.state.model_dump_json(),
            step_name=step_name,
        )
        async with self._pool.acquire() as conn:
            await conn.execute(
                """INSERT INTO session_checkpoints
                   (checkpoint_id, session_id, user_id, fsm_phase, state_json, step_name, schema_version)
                   VALUES ($1, $2, $3, $4, $5::jsonb, $6, $7)""",
                cp.checkpoint_id, cp.session_id, cp.user_id,
                cp.fsm_phase, cp.state_json, cp.step_name, cp.schema_version
            )
        return cp

    async def load_latest(self, session_id: str) -> Optional[StateCheckpoint]:
        async with self._pool.acquire() as conn:
            row = await conn.fetchrow(
                """SELECT * FROM session_checkpoints
                   WHERE session_id = $1
                   ORDER BY created_at DESC LIMIT 1""",
                session_id
            )
        if row:
            return StateCheckpoint(**dict(row))
        return None

    async def rollback_to_step(self, session_id: str, step_name: str) -> Optional[StateCheckpoint]:
        """Restore state to the last checkpoint before a specific step."""
        async with self._pool.acquire() as conn:
            row = await conn.fetchrow(
                """SELECT * FROM session_checkpoints
                   WHERE session_id = $1 AND step_name = $2
                   ORDER BY created_at DESC LIMIT 1""",
                session_id, step_name
            )
        return StateCheckpoint(**dict(row)) if row else None
```

**Checkpoint placement in the NSC pipeline â€” before vs. after:**

```python
# NSC pipeline orchestrator â€” AFTER adding checkpointing

async def run_nsc_pipeline(container: SessionContainer, checkpoints: CheckpointRepository):

    # Step 1: Decompose
    container.fsm.transition(SessionPhase.NSC_DECOMPOSING, "starting decomposition")
    container.state.nsc.sub_segment = await decomposer_agent.run(container.state.raw_user_query)
    await checkpoints.save(container, step_name="nsc_decompose_complete")  # â† checkpoint

    # Step 2: Date Tag
    container.fsm.transition(SessionPhase.NSC_DATE_TAGGING, "decomposition succeeded")
    container.state.nsc.date_metadata = await date_tagger.run(container.state.nsc.sub_segment)
    await checkpoints.save(container, step_name="nsc_date_tag_complete")   # â† checkpoint

    # Step 3: Facet Map (most likely to fail â€” Milvus dependency)
    container.fsm.transition(SessionPhase.NSC_FACET_MAPPING, "date tagging succeeded")
    try:
        container.state.nsc.complete_query_representation = await facet_mapper.run(
            container.state.nsc.sub_segment,
            container.state.nsc.date_metadata,
        )
        await checkpoints.save(container, step_name="nsc_facet_map_complete")
    except MilvusUnavailableError:
        # Rollback to date_tag_complete â€” user can retry from there, not from scratch
        container.fsm.transition(SessionPhase.FAILED, "milvus unavailable during facet map")
        raise PipelineResumeableError(
            resume_from="nsc_date_tag_complete",
            user_message="Segment lookup service is temporarily unavailable. "
                         "Your query progress has been saved â€” retry in 30 seconds."
        )

    # Step 4: Format
    container.fsm.transition(SessionPhase.NSC_FORMATTING, "facet mapping succeeded")
    container.state.nsc.segmentr_representation = await formatter.run(
        container.state.nsc.complete_query_representation
    )
    await checkpoints.save(container, step_name="nsc_format_complete")

    # Only after ALL steps succeed â€” mark segment as verified
    container.state.segment_created = True
    container.state.segment_creation_verified = True   # DSE is now allowed
    container.fsm.transition(SessionPhase.NSC_COMPLETE, "all nsc steps succeeded")
    await checkpoints.save(container, step_name="nsc_pipeline_complete")
```

**What this buys:**
- Milvus failure at step 3 no longer costs the user steps 1 and 2 â€” resume from `nsc_date_tag_complete`
- `segment_creation_verified` is only set `True` after the complete pipeline checkpoint â€” the DSE bug is impossible
- Every intermediate state is auditable in PostgreSQL â€” production debugging becomes `SELECT * FROM session_checkpoints WHERE session_id = $1`
- State snapshots are the foundation for replay-based debugging, A/B testing, and eval dataset construction

---

#### Pattern 5: State Dependency Graph with Automated Consistency Validation

**The Core Idea:** The 40+ variables have implicit dependencies that are never documented or enforced. When `FACET_CLASSIFIER_INDEX_POINTER` is advanced but `OG_FACET_CLASSIFIER_DEPENDENCY_DICT` is `None`, agents silently use `None` values causing KeyErrors. A dependency graph makes these implicit dependencies explicit and validates them before each agent runs.

```python
# state_dependencies.py â€” NEW: dependency graph for state validation

from typing import Callable, List, Optional
from dataclasses import dataclass
from state_models import SegmentationSessionState

@dataclass
class StateRequirement:
    """A named pre-condition that must hold before an agent may run."""
    name: str
    check: Callable[[SegmentationSessionState], bool]
    error_message: str

# â”€â”€ Declarative dependency registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

NSC_FACET_MAP_REQUIREMENTS: List[StateRequirement] = [
    StateRequirement(
        name="sub_segment_exists",
        check=lambda s: s.nsc.sub_segment is not None,
        error_message="FacetMapper requires sub_segment from decomposer â€” decomposer did not complete."
    ),
    StateRequirement(
        name="date_metadata_exists",
        check=lambda s: s.nsc.date_metadata is not None,
        error_message="FacetMapper requires date_metadata from date tagger â€” date tagger did not complete."
    ),
    StateRequirement(
        name="facet_catalog_loaded",
        check=lambda s: s.facet_key_identifier is not None,
        error_message="FacetMapper requires facet_key_identifier â€” catalog not loaded for this session."
    ),
]

DSE_REQUIREMENTS: List[StateRequirement] = [
    StateRequirement(
        name="segment_creation_verified",
        check=lambda s: s.segment_creation_verified is True,
        error_message="DSE cannot run â€” segment_creation_verified=False. "
                      "NSC pipeline did not complete successfully."
    ),
    StateRequirement(
        name="segment_representation_exists",
        check=lambda s: s.nsc.segmentr_representation is not None,
        error_message="DSE cannot run â€” segmentr_representation missing from NSC output."
    ),
    StateRequirement(
        name="facet_restrictions_present",
        check=lambda s: s.facet_user_restrictions is not None,
        error_message="DSE cannot run â€” facet_user_restrictions not set for this user."
    ),
]

class StateValidator:
    """Validates state meets all requirements before an agent runs."""

    @staticmethod
    def validate(state: SegmentationSessionState, requirements: List[StateRequirement]) -> None:
        failures = [
            req.error_message
            for req in requirements
            if not req.check(state)
        ]
        if failures:
            raise StatePreconditionError(
                f"State precondition failures ({len(failures)}):\n"
                + "\n".join(f"  [{i+1}] {msg}" for i, msg in enumerate(failures))
            )

class StatePreconditionError(Exception):
    """Raised when an agent's state pre-conditions are not met."""
    pass
```

**Usage â€” agents validate their preconditions before running:**

```python
# facet_mapper agent â€” AFTER adding pre-condition validation
async def run_facet_mapper(container: SessionContainer) -> Dict[str, Any]:
    # Validate all prerequisites BEFORE any LLM call
    StateValidator.validate(container.state, NSC_FACET_MAP_REQUIREMENTS)
    # Only reaches here if all requirements are met
    result = await llm_call_facet_mapper(container.state.nsc.sub_segment, ...)
    return result

# direct_segment_editor agent â€” AFTER adding pre-condition validation
async def run_dse(container: SessionContainer, edit_query: str) -> str:
    StateValidator.validate(container.state, DSE_REQUIREMENTS)  # â† catches the SegmentCreatedFlag bug
    result = await llm_call_dse(edit_query, container.state.nsc.segmentr_representation, ...)
    return result
```

---

#### Before vs. After: The SegmentCreatedFlag Bug â€” Fully Eliminated

This is the concrete production bug described in the problem statement. Here is the complete trace of how it manifests in the current system, and how it is eliminated in the new architecture:

```
CURRENT SYSTEM â€” HOW THE BUG PROPAGATES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Turn 1: User creates segment. NSC runs steps 1-3 successfully.
        Step 4 (Formatter) fails due to LLM timeout.
        Agent catches exception â€” but SEGMENT_FORMATTED_FLAG was already set to 1
        (it was set optimistically in step 3, not step 4).
        No rollback. State dict retains SEGMENT_FORMATTED_FLAG=1.

Turn 2: User retries. Router reads SEGMENT_FORMATTED_FLAG=1 â†’ routes to DSE.
        DSE looks for segment representation in state.
        SEGMENTR_REPRESENTATION is None (formatter never ran).
        DSE LLM call receives None as segment definition.
        LLM generates an edit based on None context â†’ hallucinated output.
        User sees corrupt segment. Silent failure â€” no error in logs.

NEW ARCHITECTURE â€” WHY THE BUG IS IMPOSSIBLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Turn 1: NSC runs steps 1-3. Step 4 (Formatter) fails.
        Container FSM transitions: NSC_FORMATTING â†’ FAILED.
        CheckpointRepository saves checkpoint at "nsc_facet_map_complete" (last good step).
        container.state.segment_created = False         â† never set True
        container.state.segment_creation_verified = False  â† never set True

Turn 2: User retries. Router calls fsm.can_enter_dse() â†’ False (phase=FAILED).
        Router routes to NSC resume from "nsc_facet_map_complete" checkpoint.
        Only on full pipeline completion:
          container.state.segment_creation_verified = True
          CheckpointRepository saves "nsc_pipeline_complete"
        Now fsm.can_enter_dse() â†’ True. DSE allowed.

Additionally: Even if segment_creation_verified were somehow True with no segment:
  StateValidator.validate(state, DSE_REQUIREMENTS) checks nsc.segmentr_representation is not None.
  If None â†’ raises StatePreconditionError before any LLM call.
  Three independent layers block the bug. Zero silent failures.
```

---

#### Best Practices Summary

| Practice | Current State | New Architecture | Benefit |
|----------|--------------|------------------|---------|
| **Schema validation** | None â€” any value accepted | Pydantic validators on all fields | Type errors caught at write time, not 3 turns later |
| **State machine** | Implicit flag (`SegmentCreatedFlag`) | Explicit `SessionFSM` with `VALID_TRANSITIONS` | Invalid routes raise `InvalidTransitionError` immediately |
| **Tenant isolation** | Flat dict in ADK session (pool risk) | `SessionContainer` per session, `SessionRegistry` lookup | No cross-session state bleed possible |
| **Checkpointing** | None â€” failure reruns entire pipeline | `CheckpointRepository` after each step | Resume from last good step, not from scratch |
| **Pre-conditions** | Agents assume state is correct | `StateValidator.validate()` before each agent | Agents self-document and enforce their own requirements |
| **Audit trail** | No state history | `session_checkpoints` table + FSM transition log | Every state change is queryable in PostgreSQL |
| **Rollback** | Impossible | `CheckpointRepository.rollback_to_step()` | Production incidents are recoverable |
| **Refactoring safety** | Breaking string key â†’ silent runtime error | Breaking Pydantic field â†’ IDE + mypy error at edit time | Regressions caught before commit, not in production |

---

#### Implementation Priority for This Fix

This fix can be delivered incrementally without a full pipeline rewrite:

**Week 1 â€” Pydantic models (Pattern 1):** Define `SegmentationSessionState` and all sub-models. Write a migration shim that wraps the existing flat dict and validates reads/writes. Zero behavior change â€” pure type enforcement added.

**Week 2 â€” Explicit FSM (Pattern 2):** Replace `SegmentCreatedFlag` check in the router with `SessionFSM.can_enter_dse()`. Run old and new routing in parallel for one week, log divergences, confirm zero difference. Then remove old flag.

**Week 3 â€” Checkpointing (Pattern 4):** Add `CheckpointRepository.save()` calls after each NSC step. No behavior change to the happy path â€” only adds durability for failures. Immediately eliminates the "retry from scratch" problem.

**Week 4 â€” Pre-condition validators (Pattern 5):** Add `StateValidator.validate()` at the entry point of each agent. The first time a pre-condition fails in staging, it surfaces a previously silent production bug. Fix the underlying cause.

**Week 5-6 â€” Session isolation (Pattern 3):** Refactor session creation to use `SessionContainer.new()` instead of ADK session dict. This requires the most coordination but is the safest multi-tenant foundation.

---

#### Research References for This Section

| Source | Key Finding | Application |
|--------|------------|-------------|
| [LangGraph: Low-Level Orchestration for Stateful Agents](https://docs.langchain.com/oss/python/langgraph/overview) (LangChain, 2024) | Checkpointed state graphs with thread-keyed isolation as first-class primitive | Pattern 4 (checkpointing) and Pattern 3 (session isolation) |
| [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Packer et al., UC Berkeley, 2023) | External state store enables resumable runs and cross-session memory | Pattern 4 architecture for checkpoint repository |
| [MetaGPT: Meta Programming for Multi-Agent Framework](https://arxiv.org/abs/2308.00352) (Hong et al., 2023) | Typed inter-agent contracts (SOPs) prevent error propagation between pipeline steps | Pattern 1 (Pydantic models as inter-agent contracts) and Pattern 5 (pre-condition validators) |
| [A Survey on LLM Based Autonomous Agents](https://arxiv.org/abs/2309.07864) (Xi et al., 2023) | Inter-step validation is the highest-ROI fix for sequential pipelines â€” avoids full redesign | Validates incremental fix strategy (Pattern 5 added to existing pipeline) |
| [Instructor: Structured LLM Output Library](https://python.useinstructor.com/) (Jason Liu, 2023â€“2025) | Field-level Pydantic validators with automatic retry on validation failure | Extends existing Instructor usage to state validation, not just output validation |
| [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) (Anthropic Engineering, 2024) | Evaluator-optimizer pattern â€” explicit quality gates between pipeline steps | Pattern 5 (StateValidator as pre-step quality gate) |

---

**Why it matters:** State corruption is the #1 cause of silent incorrect behavior in multi-turn AI systems. With 40+ variables, debugging production issues is extremely difficult â€” a structured state architecture reduces mean-time-to-debug from hours to minutes, and eliminates entire classes of bugs that the current architecture cannot prevent by construction.

---

### 1.3 ğŸŸ  Monolithic Prompt Architecture (No Dynamic Loading)

**Where in code:** `utils/agent_prompt.py`, `prompts/` directory (23 prompt files)

**The Problem:**
All 23 prompts are loaded from disk at startup and baked into agent configurations. There is no mechanism to:
- **A/B test** prompt variants without deploying new code
- **Roll back** to a previous prompt version if a change degrades quality
- **Per-tenant prompts** â€” all users see the same generic prompts
- **Versioned prompt history** â€” no way to know what prompt was used for a given past query

This means every prompt change requires a full code deployment, killing the ability to iterate quickly on prompt quality.

**Why it matters:** Prompt engineering is the highest-leverage improvement activity for LLM-based systems. Without dynamic loading, prompt improvement cycles take days instead of minutes.

---

### 1.4 ğŸŸ  Single LLM Provider Lock-In (Azure GPT-4o Only)

**Where in code:** `utils/adk_llm_model.py` â€” hardcoded LiteLLM config for Azure GPT-4o

**The Problem:**
The system uses a single LLM model (Azure GPT-4o) for ALL tasks â€” from simple routing decisions to complex facet mapping. This creates:

1. **No task-appropriate model routing** â€” routing a greeting through GPT-4o wastes money and time
2. **No failover** â€” if Azure gateway is down, the entire system fails
3. **No cost optimization** â€” cheaper models (GPT-4o-mini, Claude Haiku) could handle 60% of tasks
4. **Vendor lock-in** â€” switching providers requires changing core code

The Walmart LLM Gateway adds another layer of complexity â€” JWT token rotation, rate limits, and gateway-specific failures are not handled gracefully.

---

### 1.5 ğŸŸ¡ No Event-Driven Architecture (Tight Coupling)

**Where in code:** `routes/agent_routes.py` â€” synchronous request-response model (900+ lines)

**The Problem:**
The entire system runs in a synchronous request-response model. There is no event queue, pub/sub, or async pipeline. This means:
- User must wait for ALL steps to complete before seeing any results
- Long-running segment creation (30+ sub-segments) blocks the entire worker thread
- No ability to return partial results or stream step-by-step progress
- Cannot offload expensive computations to background workers

**Why it matters:** Enterprise workloads need async processing â€” users should see incremental progress, not a blank loading screen for 15+ seconds.

---

## 2. Prompt Design Flaws

### 2.1 ğŸ”´ Paraphrase-Based Prompt Mutation (Hallucination Amplifier)

**Where in code:** `prompts/facet_value_operator_mapper_prompt.txt`, `prompts/segment_decomposer_prompt.txt`

**The Problem:**
The prompts instruct the LLM to "interpret" user intent and "rephrase" sub-segment queries. This creates a chain of paraphrases where each step introduces semantic drift:

```
User: "high-value customers who shop weekly"
  â†’ Decomposer paraphrases: "customers with high purchase frequency and value"
  â†’ Date tagger paraphrases: "customers who transact at least weekly intervals"
  â†’ Facet mapper paraphrases: "transactions per 7-day rolling window"
```

By step 4, the original intent may be unrecognizable. Each paraphrase is an opportunity for hallucination.

**Fix needed:** Carry the exact user sub-query through all stages without paraphrasing. Each agent should augment (add metadata) not replace (rephrase) the user's original intent.

---

### 2.2 ğŸ”´ Ambiguity Not Resolved Before Processing

**Where in code:** `tools/dynamic_question_generation.py`, multiple agents

**The Problem:**
The system attempts clarification at multiple points in the pipeline â€” sometimes the same ambiguity is asked twice (once by the decomposer, once by the facet mapper). There is no central ambiguity registry that tracks what has been asked and resolved.

**Concrete example:**
1. Decomposer asks: "Do you mean 'recent' as in the last 30 days?"
2. User answers: "Yes, 30 days"
3. Facet mapper ALSO encounters "recent" in the same sub-query and generates another clarification question

This results in users answering the same question multiple times â€” a fundamental UX failure in enterprise systems.

**Root cause:** Each agent independently decides when to ask questions without awareness of what other agents have already asked.

---

### 2.3 ğŸŸ  Prompts Don't Use Chain-of-Thought or Structured Reasoning

**Where in code:** All prompt `.txt` files in `prompts/` directory

**The Problem:**
The prompts are instruction-based but lack structured reasoning scaffolds. They tell the LLM what to do but not how to reason about it step-by-step. Best-practice prompt engineering (especially for reasoning-heavy tasks like facet mapping) includes:

- **Step-by-step decomposition** ("First, identify entities. Then, for each entity...")
- **Self-verification steps** ("After mapping, verify that each facet exists in the catalog")
- **Explicit uncertainty handling** ("If confidence < 80%, generate a clarification question")
- **Scratchpad reasoning** ("Think through this step by step before answering")

Without these, the LLM makes mapping decisions without explicit justification, making errors hard to diagnose.

---

### 2.4 ğŸŸ  Hard-Coded Few-Shot Examples (No Dynamic Selection)

**Where in code:** `prompts/facet_value_operator_mapper_prompt.txt` (12KB), `prompts/date_extraction_prompt.txt` (10KB)

**The Problem:**
Several prompts contain hard-coded few-shot examples:
- These examples may not be representative of the current user query
- As the facet catalog evolves, examples become stale
- The most relevant examples (from historical similar queries) are not dynamically selected

**Best practice:** Use vector search to retrieve the 3-5 most similar historical segments as few-shot examples at runtime. This dramatically improves accuracy for novel query patterns.

---

### 2.5 ğŸŸ¡ No Grounding Enforcement (Citations Not Required)

**Where in code:** All prompts â€” none require citation of sources

**The Problem:**
The LLM is not instructed to cite which facets it retrieved from the vector DB vs which it generated from its own knowledge. This means:
- Cannot detect when the LLM "invents" facet names not in the catalog
- Explainability is limited â€” users can't see why a particular facet was chosen
- Hallucinated facets pass through silently and cause downstream errors

**Fix needed:** Require the LLM to explicitly cite retrieved facet names as `[RETRIEVED: FacetName]` vs inferred ones as `[INFERRED: value]` and reject inferred outputs at the validator level.

---

### 2.6 ğŸŸ¡ System Prompt Too Large (Context Window Pressure)

**Where in code:** `prompts/facet_value_operator_mapper_prompt.txt` (12KB), `prompts/route_agent_prompt.txt`

**The Problem:**
Some prompts are 10-12KB of text. For complex queries with many sub-segments, the combined context (system prompt + conversation history + facet catalog + user query + retrieved values) can approach or exceed 32K tokens, causing:
- Truncation of earlier conversation history
- "Lost in the middle" attention failures (LLMs attend poorly to middle of very long contexts)
- Increased cost per query

---

## 3. Evaluation Gaps

### 3.1 ğŸ”´ No Production Eval Gates (Prompts Deploy Without Testing)

**Where in code:** `evaluations/` directory â€” evaluation framework exists but is decoupled from CI/CD

**The Problem:**
The system has an evaluation framework (CLI, Streamlit UI, reports) but it is **not integrated with the deployment pipeline**. This means:
- Prompt changes can be deployed to production without passing eval tests
- No automated regression detection
- No quality baseline that must be maintained
- Evaluations are run manually and infrequently

**Enterprise standard:** Every prompt change, every model update, every skill modification must pass an eval gate before reaching production. The current architecture has no such gate.

---

### 3.2 ğŸŸ  Eval Dataset Is Manually Created (Not From Real Queries)

**Where in code:** `evaluations/data/`, `evaluations/eval_sets/`

**The Problem:**
The evaluation test sets are hand-crafted rather than derived from real user queries. This creates:
- **Distribution mismatch** â€” eval queries may not reflect actual production query patterns
- **Coverage gaps** â€” unusual but important query patterns may not be tested
- **Staleness** â€” eval sets don't grow as user behavior evolves
- **Goodhart's Law risk** â€” the system may optimize for eval metrics that don't reflect real quality

**Best practice:** Continuously mine production queries, cluster them, and automatically sample representative queries for eval sets. This ensures eval distribution matches production distribution.

---

### 3.3 ğŸŸ  No Regression Detection Between Versions

**Where in code:** `evaluations/` â€” no version comparison or diff metrics

**The Problem:**
When a new prompt version is tested, there is no automated comparison against the previous version:
- Cannot detect "this change improved date parsing by 5% but degraded facet mapping by 12%"
- No A/B test framework for prompts
- No statistical significance testing for evaluation improvements

---

### 3.4 ğŸŸ¡ Evals Don't Cover Edge Cases or Adversarial Inputs

**Where in code:** `evaluations/eval_sets/` â€” standard positive test cases only

**The Problem:**
The evaluation suite covers happy-path scenarios but lacks:
- **Adversarial inputs** (gibberish, SQL injection, out-of-scope queries)
- **Edge cases** (empty segments, conflicting conditions, fiscal year boundary queries)
- **Multilingual inputs** (non-English queries)
- **Very long queries** (complex multi-condition segments)
- **Ambiguous intent** (queries that could validly be interpreted multiple ways)

---

## 4. Scalability Limits

### 4.1 ğŸ”´ Database Connection Pool Exhaustion (Max 30 Concurrent Users)

**Where in code:** `database/connection.py` â€” `POOL_MIN=1, POOL_MAX=3`

**The Problem:**
With 10 Gunicorn workers and max 3 DB connections per worker, the system supports a maximum of **30 concurrent database connections**. Beyond that, queries queue and timeout.

For an enterprise platform supporting marketing teams, data analysts, and CRM teams simultaneously, 30 concurrent connections is completely insufficient.

**Actual scalability ceiling:**
- 10 workers Ã— 3 connections = 30 max concurrent operations
- Each segment creation takes 3-5 DB writes
- At 30 concurrent users, p99 latency spikes to 30+ seconds

---

### 4.2 ğŸŸ  No Caching Layer (Every Request Is Cold)

**Where in code:** No Redis, Memcached, or in-memory cache anywhere in the codebase

**The Problem:**
The system performs expensive operations on every request with no caching:
1. **Facet catalog loading** â€” loaded from pickle on startup but re-checked every request
2. **Milvus searches** â€” same query for "California" searches Milvus every time
3. **Embedding generation** â€” BGE encodes strings on every call with no memoization
4. **LLM responses** â€” no caching of identical sub-queries
5. **Segment size estimates** â€” no caching of recently computed estimates

**Conservative estimate:** 40-60% of Milvus search calls and 20-30% of embedding computations could be served from cache.

---

### 4.3 ğŸŸ  Entire Facet Catalog Loaded Into Memory

**Where in code:** `utils/metadata.py` â€” loads full catalog as pickle at startup

**The Problem:**
The facet catalog (all available segmentation attributes and their valid values) is loaded entirely into memory at startup. As the catalog grows:
- Memory footprint increases linearly
- Startup time increases
- New facets require restarting the service
- Multi-tenant support requires loading tenant-specific catalogs (multiplies memory usage)

**Enterprise need:** Catalog should be lazily loaded on demand with intelligent prefetching and per-tenant namespacing.

---

### 4.4 ğŸŸ¡ No Request Queue or Backpressure

**Where in code:** `api.py`, `routes/agent_routes.py`

**The Problem:**
When traffic spikes, requests accumulate in the Uvicorn/Gunicorn queue with no backpressure. This creates:
- Cascading failures during peak load
- No request prioritization (urgent vs. batch requests treated equally)
- No graceful degradation (system should return faster, simpler responses under load)

---

## 5. Reliability Problems

### 5.1 ğŸ”´ Milvus Is a Single Point of Failure

**Where in code:** `utils/milvus.py` â€” all facet/value search goes through one Milvus instance

**The Problem:**
If Milvus is unavailable:
- The entire NSC pipeline fails
- There is no fallback to traditional keyword search or SQL-based facet lookup
- Users see a hard error with no useful degraded behavior

**Enterprise standard:** Every external dependency must have a fallback. For Milvus, the fallback should be fuzzy string matching against the in-memory facet catalog.

---

### 5.2 ğŸŸ  No Segment Validation Before Persistence

**Where in code:** `sub_agents/new_segment_creation/sub_agents/segment_format_generator/agent.py`

**The Problem:**
After the LLM generates a segment definition, it is saved to PostgreSQL **without validation**:
- Facet names are not verified against the actual database schema
- Values are not validated against enumerated options
- Boolean logic (INCLUDE/EXCLUDE rules) is not validated for correctness
- Segments may be syntactically valid JSON but semantically invalid queries

**Result:** Users receive confirmation that their segment was created, but the segment silently fails when executed downstream (in CRM, email campaign, etc.).

---

### 5.3 ğŸŸ  Conversation History Truncation Causes Context Loss

**Where in code:** `routes/agent_routes.py` â€” conversation history management

**The Problem:**
For long multi-turn conversations (users who refine segments many times), the conversation history is truncated when it exceeds the token limit. The truncation strategy is naive:
- Oldest messages are dropped first
- The original segment creation intent may be lost
- The user's original query constraints disappear, causing the editor to lose context

**Why it matters:** A user who spent 10 minutes refining a complex segment shouldn't lose their original intent because the context window filled up.

---

### 5.4 ğŸŸ¡ Retry Logic Is Primitive

**Where in code:** `utils/pydantic_infero.py` â€” `max_tries=3` with no exponential backoff

**The Problem:**
When LLM calls fail or return invalid structured output, the retry logic:
- Retries exactly 3 times with no backoff
- Doesn't distinguish between recoverable errors (rate limit) and non-recoverable (invalid schema)
- Doesn't pass error context to the LLM on retry ("Your previous response was invalid because...")
- May retry after the LLM gateway already returned 429 (rate limit), worsening the situation

---

## 6. Missing Capabilities

### 6.1 ğŸ”´ No Long-Term Memory or Pattern Learning

**Current state:** The system has a session persistence layer (PostgreSQL) that stores conversation history per session, but there is **no cross-session learning or pattern memory**.

**What's missing:**
- The system doesn't remember that "this user always means 30-day windows when they say 'recent'"
- Successful segment patterns from previous sessions are not surfaced as suggestions
- User-specific vocabulary preferences are not learned over time
- Common segment templates from power users are not available to all users

**Enterprise impact:** Power users must re-specify the same preferences every session. New users can't benefit from best practices discovered by experienced users.

---

### 6.2 ğŸ”´ No Multi-Tenant Support

**Current state:** Single-tenant architecture. All users share the same:
- Facet catalog
- Prompts
- Configuration
- Segment namespace

**What's missing:**
- Per-tenant facet catalogs (Tenant A's "high value" = $500+, Tenant B's = $100+)
- Per-tenant prompt customizations (different terminology, different output formats)
- Tenant data isolation (Tenant A can't see Tenant B's segments)
- Per-tenant usage limits and billing

---

### 6.3 ğŸŸ  No Hypothesis Assessment Capability

**Current state:** The system creates and edits segments. It does not evaluate whether a segment hypothesis is sound.

**What's missing:**
- "Is this segmentation strategy likely to drive the user's stated business goal?"
- "Are these segment boundaries creating meaningful separation?"
- "Based on historical campaign data, which segment characteristics drive highest conversion?"
- "This segment overlap with Segment X by 73% â€” would you like to merge them?"

---

### 6.4 ğŸŸ  No Auto-Improvement Pipeline

**Current state:** Prompts are static files, manually updated by engineers.

**What's missing:**
- Feedback collection from users (thumbs up/down on segment quality)
- Automated analysis of which segment types have high correction rates
- Prompt optimization based on failure patterns (DSPy-style optimization)
- Automatic few-shot example generation from successful query-segment pairs
- A/B testing of prompt improvements

---

### 6.5 ğŸŸ¡ No Model Exploration or Recommendation Engine

**Current state:** The system creates segments based on user queries. It does not:
- Analyze the quality of existing segments
- Suggest when a rule-based segment should become an ML model
- Identify patterns that suggest model-based targeting would outperform rule-based targeting
- Recommend segment consolidation or expansion based on overlap analysis

---

### 6.6 ğŸŸ¡ No Audit Trail or Explainability

**Current state:** Segments are stored in PostgreSQL but without:
- A record of WHY each facet was chosen
- Which retrieved values from Milvus influenced the decision
- Which version of which prompt generated the segment
- A human-readable explanation of the segment's business rationale

**Enterprise need:** Compliance and marketing teams need to understand and audit why a segment was defined as it was.

---

## 7. Severity Summary Table

| # | Bottleneck | Category | Severity | Business Impact |
|---|-----------|----------|----------|----------------|
| 1.1 | Sequential agent pipeline | Architecture | ğŸ”´ Critical | 2x query latency |
| 1.2 | 40+ variable state machine | Architecture | ğŸ”´ Critical | Silent corruptions |
| 1.3 | Monolithic prompt architecture | Architecture | ğŸŸ  High | No rapid iteration |
| 1.4 | Single LLM provider | Architecture | ğŸŸ  High | Cost + resilience |
| 1.5 | No event-driven architecture | Architecture | ğŸŸ¡ Medium | No streaming |
| 2.1 | Paraphrase-based prompt mutation | Prompts | ğŸ”´ Critical | Hallucinations |
| 2.2 | Duplicate clarification questions | Prompts | ğŸ”´ Critical | UX failure |
| 2.3 | No structured reasoning in prompts | Prompts | ğŸŸ  High | Lower accuracy |
| 2.4 | Static few-shot examples | Prompts | ğŸŸ  High | Stale examples |
| 2.5 | No grounding enforcement | Prompts | ğŸŸ¡ Medium | Undetected hallucination |
| 2.6 | Oversized system prompts | Prompts | ğŸŸ¡ Medium | Context pressure |
| 3.1 | No production eval gates | Evaluation | ğŸ”´ Critical | Regressions ship |
| 3.2 | Manual eval datasets | Evaluation | ğŸŸ  High | Coverage gaps |
| 3.3 | No regression detection | Evaluation | ğŸŸ  High | No A/B testing |
| 3.4 | No adversarial eval coverage | Evaluation | ğŸŸ¡ Medium | Fragile edge cases |
| 4.1 | DB connection pool exhaustion | Scalability | ğŸ”´ Critical | Max 30 concurrent |
| 4.2 | No caching layer | Scalability | ğŸŸ  High | 2x unnecessary compute |
| 4.3 | Full catalog in memory | Scalability | ğŸŸ  High | Memory grows linearly |
| 4.4 | No request queue | Scalability | ğŸŸ¡ Medium | Cascading failures |
| 5.1 | Milvus single point of failure | Reliability | ğŸ”´ Critical | System-wide outage |
| 5.2 | No segment validation | Reliability | ğŸŸ  High | Silent failures |
| 5.3 | History truncation | Reliability | ğŸŸ  High | Context loss |
| 5.4 | Primitive retry logic | Reliability | ğŸŸ¡ Medium | Cascades under load |
| 6.1 | No long-term memory | Missing | ğŸ”´ Critical | No learning |
| 6.2 | No multi-tenant support | Missing | ğŸ”´ Critical | Enterprise blocker |
| 6.3 | No hypothesis assessment | Missing | ğŸŸ  High | No strategic value |
| 6.4 | No auto-improvement | Missing | ğŸŸ  High | Static quality |
| 6.5 | No model recommendation | Missing | ğŸŸ¡ Medium | Missed opportunity |
| 6.6 | No audit trail | Missing | ğŸŸ¡ Medium | Compliance risk |

---

## 8. Root Cause Map

The vast majority of identified bottlenecks trace back to **three root causes**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ROOT CAUSE 1                                  â”‚
â”‚        "Built for Demo, Not for Production Scale"                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Symptoms:                                                            â”‚
â”‚  â€¢ No caching â†’ every request is cold                               â”‚
â”‚  â€¢ No request queue â†’ cascading failures under load                 â”‚
â”‚  â€¢ DB pool too small â†’ max 30 concurrent users                      â”‚
â”‚  â€¢ Sequential pipeline â†’ 2x longer latency than needed              â”‚
â”‚  â€¢ Single Milvus instance â†’ no redundancy                           â”‚
â”‚                                                                      â”‚
â”‚ Fix: Infrastructure hardening + caching + async pipeline            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ROOT CAUSE 2                                  â”‚
â”‚         "Prompt Engineering Without a System"                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Symptoms:                                                            â”‚
â”‚  â€¢ Static prompts deployed without eval gates                       â”‚
â”‚  â€¢ Prompts paraphrase instead of preserve user intent               â”‚
â”‚  â€¢ No few-shot retrieval â€” examples are hard-coded                  â”‚
â”‚  â€¢ Ambiguity not centrally tracked, asked multiple times            â”‚
â”‚  â€¢ No grounding enforcement â€” hallucinations undetected             â”‚
â”‚                                                                      â”‚
â”‚ Fix: Eval-first development + dynamic prompt loading +              â”‚
â”‚      grounding enforcement + centralized ambiguity resolver         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ROOT CAUSE 3                                  â”‚
â”‚           "Agent That Doesn't Learn or Remember"                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Symptoms:                                                            â”‚
â”‚  â€¢ No cross-session memory â†’ users repeat preferences               â”‚
â”‚  â€¢ No pattern learning â†’ same errors repeat                         â”‚
â”‚  â€¢ No auto-improvement â†’ prompts stagnate                           â”‚
â”‚  â€¢ No multi-tenant adaptation â†’ one-size-fits-all                  â”‚
â”‚  â€¢ No hypothesis evaluation â†’ just executes, doesn't advise        â”‚
â”‚                                                                      â”‚
â”‚ Fix: Memory layer (short-term + long-term) + feedback loops +       â”‚
â”‚      prompt optimization pipeline + tenant personalization          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What's Working Well (Do Not Break)

Before upgrading, preserve these strengths:

| Strength | Detail |
|----------|--------|
| âœ… Structured LLM Output | Pydantic + Instructor provides reliable typed responses |
| âœ… Milvus Semantic Search | Vector search for facet discovery is accurate and fast |
| âœ… Multi-step Agent Decomposition | Breaking complex queries into sub-segments is effective |
| âœ… Interactive Clarification | Multi-turn dialogue for ambiguity resolution works |
| âœ… Evaluation UI | Streamlit-based evaluation creator and runner is a great foundation |
| âœ… Phoenix Tracing | Arize Phoenix integration provides good observability baseline |
| âœ… PostgreSQL Session Persistence | Cross-session conversation continuity |
| âœ… Docker/K8s Deployment | Production-grade container infrastructure |

---

## Priority Order for Resolution

Based on business impact and interdependency analysis:

**Immediate (Week 1-2):**
1. Production eval gates â€” stop shipping regressions
2. Milvus fallback â€” stop single-point-of-failure outages
3. Segment validation â€” stop silent failures

**Short-term (Week 3-6):**
4. Caching layer (Redis) â€” 40% latency reduction
5. DB connection pool increase â€” support >30 concurrent users
6. Central ambiguity resolver â€” eliminate duplicate questions
7. Grounding enforcement â€” detect hallucinations

**Medium-term (Week 7-12):**
8. Dynamic prompt loading â€” enable rapid iteration
9. Parallel agent execution â€” reduce latency further
10. Long-term memory system â€” enable cross-session learning
11. Multi-tenant support â€” enterprise customer requirement

**Long-term (Month 4-6):**
12. Auto-improvement pipeline â€” prompts that get better with use
13. Hypothesis assessment engine â€” strategic advisory capability
14. Model exploration recommendations â€” ML vs rule-based guidance

---

*Document produced as part of Enterprise Agentic Research â€” Research 1 (Sonnet Claude)*
*All analysis based on direct codebase inspection of Smart-Segmentation at `/Users/s0m0ohl/customer_segement/Smart-Segmentation`*
