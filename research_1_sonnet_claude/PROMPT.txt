================================================================================
ENTERPRISE AGENTIC RESEARCH - RESEARCH 1 (SONNET CLAUDE)
================================================================================
Research ID    : research_1_sonnet_claude
Source Code    : /Users/s0m0ohl/customer_segement/Smart-Segmentation
Output Folder  : /Users/s0m0ohl/customer_segement/enterprise_agentic_research/research_1_sonnet_claude
Reference      : /Users/s0m0ohl/customer_segement/ditto_delivery (for structure reference)
================================================================================


MISSION
--------------------------------------------------------------------------------
Deeply analyze the Smart-Segmentation codebase, then research and propose a
concrete upgrade plan to transform it into an enterprise-grade agentic customer
segmentation system. The system must be reliable, scalable, auto-improving, and
deliver state-of-the-art quality while remaining cost-conscious.


PART A: CODEBASE ANALYSIS
--------------------------------------------------------------------------------
Carefully study the Smart-Segmentation code at:
  /Users/s0m0ohl/customer_segement/Smart-Segmentation

Evaluate it against these upgrade criteria:
  1. Best thinking models (structured reasoning, chain-of-thought)
  2. Flexible, pluggable architecture (skills, tools, knowledge as modules)
  3. Eval-first development (evaluation gates before deployment)
  4. Auto-improvement (feedback loops, prompt optimization)
  5. Memory (short-term conversation state + long-term learned patterns)
  6. Multi-tenant support (tenant-specific configs, data isolation)
  7. High quality with consistent, reliable, accurate results
  8. Scalable to support deep customer insights (marketing campaigns,
     CRM campaigns, segment expansion, etc.)
  9. Hypothesis assessment (evaluate user hypotheses, suggest better ideas
     with reasoning)
  10. Model exploration and recommendation (assess existing models, suggest
      building new ones with concrete logs and memory-based evidence)


PART A2: GROUND TRUTH DATA & FACET RETRIEVAL ANALYSIS
--------------------------------------------------------------------------------
The file below is the ground truth dataset used to evaluate segmentation quality.
Analyze it carefully to understand what it contains and what it implies about
the retrieval strategy currently in use.

  File: /Users/s0m0ohl/customer_segement/ground_truth_cdi(Updated Ground Truth).csv
  Also at: Smart-Segmentation/metadata/segment-historical-dataset/input/ground_truth_cdi.csv

  Columns (14 total, ~18,779 rows):
    Seg No, Segment Name, Updated Segment Description with Add-on,
    expected facets, updated expected facets, Remarks on values,
    Updated Segment Definition, Predicted CDI, Predicted E2F,
    Segment Description, Add-On Description, Segment Definition,
    OLD Updated Segment Description, OLD Segment Description with Add-on

  Key code context:
    - The system currently embeds 500+ facets (names + values) into Milvus
      vector DB (BGE or MiniLM models) and retrieves them per sub-segment query.
    - Two Milvus collections are used: one for facet names, one for facet values.
    - Hybrid search with RRF ranker is already partially implemented in milvus.py.
    - Facet catalog is loaded from BigQuery / pickle files (email_mobile vs cbb_id
      keys), and restrictions are applied as a filter on top of vector search.
    - After retrieval, a separate LLM (facet_classifier_matcher_prompt.txt) picks
      the best refinement value for each shortlisted facet.
    - The ground truth CSV contains expected vs. predicted facets used for evals.

  SAMPLE DATA (3 representative rows — read the full file for complete analysis):

  Row 1:
    Segment Name          : Test-Segment-WomenFashion
    Description           : Build a segment for spring fashion shoppers looking to
                            buy women's clothing and shoes possibly from Free Assembly.
    expected facets       : Persona | Propensity Super Department Strict |
                            Propensity Brand Strict
    updated expected facets: Persona | Propensity Super Department |
                            Propensity Division | Propensity Brand
    Remarks on values     : Removed Propensity Super Department = "ACCESSORIES"
    Predicted CDI         : (JSON blob — sessionId=2, context matches description)
    Predicted E2F         : (JSON blob — sessionId=2, facet-operator-value mapping)

  Row 2:
    Segment Name          : Test-Segment-SpringCleaners
    Description           : Build a segment for spring cleaners who want to get their
                            yard and patio clean and fixed up with push engagement levels
    expected facets       : CRM Push Engagement | Persona |
                            Propensity Super Department Strict | Propensity Super Department
    updated expected facets: CRM Push Engagement | Persona |
                            Propensity Super Department | Propensity Division
    Remarks on values     : (empty)
    Predicted CDI/E2F     : sessionId=1

  Row 3:
    Segment Name          : Test-Segment-BabyPlanners
    Description           : Build a segment for new parents who may have an active baby
                            registry status and email engagement level between 0-90 days
    expected facets       : CRM Email Engagement | Email Savings and Updates Opt-in |
                            Persona | Propensity Super Department | Propensity Brand |
                            Baby Registry Status
    updated expected facets: CRM Email Engagement | Persona | Propensity Super Department |
                            Propensity Brand | Propensity Division | Baby Registry Status
    Remarks on values     : (empty)

  KEY OBSERVATIONS FROM SAMPLE DATA:
    - "expected facets" vs "updated expected facets" represent older vs. newer ground
      truth — the "Strict" suffix variants (e.g., "Propensity Super Department Strict")
      were deprecated in favor of non-strict versions; this means the facet name
      retrieval must handle synonym/alias resolution (Strict vs. non-Strict).
    - The "Predicted CDI" and "Predicted E2F" columns hold full JSON prediction
      outputs from earlier model runs — compare against expected to measure accuracy.
    - "Remarks on values" documents manual corrections made to expected facets —
      these remarks are a gold mine for understanding edge cases and failure modes.
    - The Segment Definition column contains the raw JSON segment definition output
      (the final formatted output) — analyze for structural correctness and
      completeness vs. what the description asked for.
    - With 18,779 rows, compute aggregate statistics: how many rows have non-empty
      Remarks (indicating a correction was needed)? How often does Predicted CDI
      match expected facets exactly? What is the average number of expected facets
      per segment (complexity metric)?

  Required analysis tasks (include findings in 01_bottleneck_analysis.md and
  recommendations in 03_concrete_upgrade_proposal.md):

  A. IS EMBEDDING-BASED RAG ACTUALLY THE RIGHT CHOICE FOR 500+ FACETS?
     Analyze whether vector similarity search (Milvus + BGE/MiniLM) is the
     optimal retrieval strategy for a structured, finite facet catalog of
     ~500+ items. Specifically:
       - These facets are NOT free-text KB articles — they are structured
         catalog entries (name, description, type, l1_value list, metadata).
         Does dense vector retrieval add noise vs. exact/structured lookup?
       - Compare: vector search vs. BM25/keyword vs. structured SQL/filter
         lookup vs. LLM-with-full-catalog-in-context vs. hybrid approaches.
       - At what catalog size does each approach break down in terms of
         precision, recall, latency, and cost?
       - Given the facet catalog has types (numeric, date, string, csv),
         restrictions (tenant-level), and hierarchy (parent/child), could
         a structured query + metadata filter replace or outperform embedding
         search entirely for most facets?
       - When is embedding search genuinely needed (e.g., semantic synonym
         matching, brand name inference, vague user queries)?

  B. GROUND TRUTH CSV: WHAT IT REVEALS ABOUT CURRENT QUALITY & GAPS
     Analyze the CSV data to understand:
       - How large is the gap between expected_facets and predicted facets?
         Where does the system fail (recall, precision, wrong values)?
       - Are failures concentrated in specific facet types (date, numeric,
         propensity, persona, purchase-based) or query types?
       - What does the Remarks column reveal about systematic failure modes?
       - Does the CDI/E2F prediction data expose any patterns?
       - Use these findings to ground the bottleneck analysis in real data.

  C. BETTER THAN PURE EMBEDDING: ALTERNATIVES AND ENHANCEMENTS
     Research and recommend what is better than pure embedding-based retrieval
     for this specific use case (structured facet catalog with metadata):
       - Structured lookup first + embeddings only as fallback (cascade approach)
       - Named entity recognition pre-pass to map query terms to exact catalog
         entries before invoking vector search (NER is already implemented —
         analyze if it is being used effectively)
       - LLM-as-ranker: after shortlisting N candidates, use LLM to pick the
         best match using full facet description (current approach partially
         does this via facet_classifier_matcher_prompt.txt — is it optimal?)
       - Ontology / taxonomy-aware retrieval: facets have categories and
         hierarchies (super department, brand, propensity tier) — can a
         structured taxonomy graph replace vector search for those dimensions?
       - Fine-tuned embedding model on domain-specific facet pairs vs.
         generic BGE/MiniLM — what would it take, and is it worth it?
       - Function-calling / tool-per-facet-type: give the LLM typed tools
         (search_propensity_facets, search_purchase_facets, search_persona_facets)
         so it performs structured, type-aware retrieval instead of one big
         semantic search across all 500+ facets.

  D. MULTI-TENANT: NEW TENANT WITH A SIMILAR BUT DIFFERENT DATASET
     The primary multi-tenant concern is NOT just scale — it is onboarding a
     new tenant who brings a dataset that is structurally similar to the current
     one but uses different facet names, different ground truth rows, different
     contextual descriptions, and potentially different domain vocabulary.
     Example: current tenant = Walmart (email_mobile, cbb_id facet catalogs).
     New tenant = a similar retailer with ~400-600 facets, their own ground
     truth CSV (same 14-column structure), their own refinement descriptions,
     but different facet names, different propensity categories, different brand
     lists, and different business rules.

     The system must be READY for this WITHOUT requiring:
       - Code changes to the pipeline stages
       - Rewriting any static prompts
       - Re-training or fine-tuning any embedding model
       - Changes to the core agent architecture

     Required research and recommendations — cover in both bottleneck analysis
     (what is currently hardcoded/tenant-specific) and upgrade proposal
     (how to make it tenant-agnostic):

     D1. WHAT IS CURRENTLY HARDCODED PER TENANT (IDENTIFY IN CODE):
       - contextual_information/contextual_information_on_refinements.txt —
         this is injected into facet_classifier_matcher_prompt.txt as static
         text; it is specific to the current tenant's refinement vocabulary.
         A new tenant would need their own version. Is this swappable at
         runtime today? It is not — it is loaded as a static file. Flag this
         as a critical hardcoded bottleneck.
       - contextual_information/catalog_view_description.txt — same issue;
         describes current tenant's catalog structure.
       - contextual_information/segment_decomposer_hints.txt and
         facet_value_mapper_hints.txt — these are tenant-specific hint files
         injected into prompts. A new tenant's domain may require different
         hints.
       - contextual_information/channel_date_attribute_map.json — maps
         purchase channels (ONLINE, STORE) to date facet names; these names
         are tenant-specific.
       - The ground truth CSV is only used for eval today — it is NOT used
         at runtime as few-shot context. A new tenant's ground truth CSV
         would need to be onboarded into the eval framework, but also into
         runtime few-shot retrieval if that pattern is adopted.
       - Milvus collections are named with tenant identifiers in env vars
         (MILVUS_FACET_NAME_COLLECTION, MILVUS_FACET_VALUE_COLLECTION) —
         these are runtime-configurable, which is good. But the collection
         schema and embedding model are shared.

     D2. TENANT ONBOARDING BLUEPRINT — WHAT SHOULD THE ARCHITECTURE SUPPORT:
       Research and recommend a concrete onboarding flow so that a new tenant
       with a similar dataset can be onboarded without code changes:
         - Tenant config file / manifest: defines tenant ID, facet catalog
           source (BigQuery table / pickle / CSV), Milvus collection names,
           contextual info file paths, ground truth CSV path, embedding model,
           and any tenant-specific restrictions or business rules.
         - Tenant-scoped contextual info store: each tenant has their own
           set of contextual_information files, loaded at runtime based on
           tenant ID. Research how to make this dynamic.
         - Tenant-scoped ground truth: each tenant has their own ground truth
           CSV. Research how to: (a) run per-tenant evals without mixing data,
           and (b) use that CSV as dynamic few-shot context at inference time
           (retrieve the closest historical segment definition for this tenant).
         - Tenant-scoped Milvus partition or collection: should each tenant
           get a separate collection (strong isolation, higher infra cost) or
           a shared collection with tenant_id metadata filter (lower cost,
           risk of cross-tenant bleed)? Research trade-offs at 2-10 tenants.
         - Zero-code tenant vocabulary adaptation: when a new tenant's facet
           names are different (e.g., "Affinity Score" instead of "Propensity"),
           how does the system learn this mapping without prompt surgery?
           Research: embedding-based alias resolution, tenant synonym tables,
           LLM-generated vocabulary bridge at onboarding time.
         - Per-tenant prompt hints and contextual info injection: the hints
           files (decomposer_hints, fvom_hints, refinements context) must
           be generatable or customizable per tenant. Research: can an LLM
           generate these hint files automatically from a new tenant's facet
           catalog + a few labeled examples? What is the minimum human input
           needed to onboard a new tenant?

     D3. QUALITY PARITY ACROSS TENANTS:
       Research how to ensure a new tenant gets the same LLM output quality
       as the primary tenant even with:
         - Fewer ground truth examples (new tenants start with less history)
         - Different facet vocabulary (no synonym overlap with existing model)
         - No tenant-specific fine-tuning (must work zero-shot or few-shot)
       Techniques to research:
         - Cross-tenant few-shot transfer: can the primary tenant's ground
           truth rows that share similar segment types (persona, propensity,
           date-based) be reused as few-shot examples for a new tenant?
         - Universal facet taxonomy: can a shared abstract taxonomy
           (persona, propensity, engagement, purchase, registry, date) bridge
           tenant-specific naming so the LLM reasons at the abstract level
           and maps to tenant-specific names at retrieval time?
         - Tenant cold-start strategy: minimum viable dataset a new tenant
           must provide (e.g., 50 labeled segment→facet pairs) to reach
           acceptable quality. Research evidence from enterprise RAG cold-start
           literature.

     D4. SCALE READINESS (SIZE IS NOT THE PRIMARY CONCERN, BUT NOTE IT):
       Once the above tenant-agnostic architecture is in place, also address
       what changes if a tenant's catalog is moderately larger (~800-1500
       facets, not just 500). The system should handle this without special
       casing — the tenant config, partitioning, and retrieval approach should
       scale naturally. Identify the thresholds where additional engineering
       is needed vs. where the architecture handles it transparently.


PART A3: PIPELINE STAGE ANALYSIS — DO WE NEED THEM?
--------------------------------------------------------------------------------
The current Smart-Segmentation pipeline has these distinct LLM-call stages:

  Stage 1 - Route Agent         (route_agent_prompt.txt)
  Stage 2 - Segment Decomposer  (segment_decomposer_prompt.txt)
  Stage 3 - Date Tagger         (date_extraction_prompt.txt)
  Stage 4 - Facet/Value Mapper  (facet_value_operator_mapper_prompt.txt)
  Stage 5 - Facet Classifier    (facet_classifier_matcher_prompt.txt)
            + Linked Facet Matcher (linked_facet_matcher_prompt.txt)
            + Ambiguity Resolver  (ambiguity_framework_prompt.txt)
  Stage 6 - Formatter           (master_format_generator_prompt.txt /
                                  new_segment_formatter_delegation_prompt.txt)
  Stage 7 - Direct Segment Editor (direct_segment_editor_prompt.txt)

  Also present: NER agent, summary generator, purchase facets helper,
  facet_delete/replace/value_add/value_delete/value_replace prompts.

  Required analysis and research (include in both 01_bottleneck_analysis.md
  and 03_concrete_upgrade_proposal.md):

  A. IS THIS STAGE DECOMPOSITION THE RIGHT ARCHITECTURE?
     Research and analyze:
       - What is the cost (latency, LLM API calls, error propagation) of
         this many-stage pipeline vs. a single capable LLM call?
       - Can a modern long-context reasoning model (Claude Opus, GPT-4o,
         Gemini 1.5 Pro) collapse stages 2–5 into a single structured
         output call with the right prompt + tool definitions?
       - Which stages are genuinely necessary as separate LLM calls because
         the task requires different context or reasoning?
       - Which stages could be deterministic code (no LLM needed) — e.g.,
         date parsing (Stage 3 uses date_tagger_patterns.json, could be pure
         regex/rule-based), format conversion (Stage 6 is pure JSON mapping)?
       - What is the error accumulation risk of a 7-stage chain vs. a
         2-3 stage design?

  B. LLM GENERATE INSTRUCTIONS ON THE FLY vs. STATIC STAGED PROMPTS
     Research the trade-off between:
       - Current approach: many static, specialized prompts for each stage,
         each with hardcoded logic and examples (brittle, hard to update)
       - Alternative A: Single agent with a KB article / skill per operation
         type (decompose, date-extract, map-facets, format) loaded dynamically
         via RAG — LLM selects which skill to apply on the fly
       - Alternative B: LLM generates its own execution plan (planning agent)
         and calls typed tools (search_facets, tag_dates, format_output) in
         the order it determines is correct for the specific query
       - Alternative C: Hybrid — static system prompt defines the contract
         (input/output schema, grounding rules), LLM picks tools dynamically
         within that contract (most aligned with Part D Section 3 above)
     For each alternative research: quality benchmarks, failure modes,
     observability, and ease of updating business logic without prompt surgery.

  C. STATIC PROMPT + KB ARTICLE AGENT PATTERN
     Analyze whether the current pipeline could be refactored as:
       - One static core agent (identity, loop, grounding rules, output schema)
       - KB articles for each "skill" (how to decompose a segment, how to
         handle dates, how to pick facets, how to format output)
       - Ground truth CSV rows as few-shot examples retrieved dynamically
         (RAG over ground truth — retrieve similar past segment definitions
         as few-shot context rather than hardcoding examples in every prompt)
     Research evidence for and against this pattern from enterprise agent
     implementations. Does it improve maintainability and quality?

  D. DELIBERATE CODE-LEVEL RESEARCH TARGETS
     While analyzing the code, specifically investigate and research:
       - agentic_framework/utils/milvus.py: hybrid search with RRFRanker is
         implemented but may not be the default path — research whether hybrid
         BM25+dense consistently outperforms single-index dense search for
         short structured queries like facet names
       - agentic_framework/utils/embedding.py: BGE vs MiniLM selection at
         runtime — research which is better for short domain-specific queries
         (facet names are typically 2-6 words); is a domain-adapted model
         worth the investment?
       - agentic_framework/sub_agents/new_segment_creation/tools/
         shortlist_generation.py: analyze the cascaded search logic
         (name search → value search → purchase facets path → date facets
         path) — is this the right cascade order? What are the failure modes?
       - agentic_framework/contextual_information/: catalog_view_description.txt,
         contextual_information_on_refinements.txt — these are injected as
         static context into prompts. Research if dynamic retrieval of only
         the relevant contextual info would be better
       - agentic_framework/prompts/date_extraction_prompt.txt +
         date_tagger_patterns.json: is a full LLM stage needed for date
         extraction, or could a rule-based + lightweight NLP approach
         (dateparser, spaCy) handle 95% of cases?
       - The ground truth CSV is only used for eval — research whether it
         should be actively used at runtime as few-shot context (RAG over
         historical segment definitions)


PART B: DEEP RESEARCH
--------------------------------------------------------------------------------
Conduct thorough research on the latest approaches for building enterprise-level
AI agents. This is NOT about personal assistant agents (like Ralph Loops, Open
Claw, or similar community projects). Focus specifically on enterprise-grade
agent architectures.

Research sources should include:
  - Academic papers
  - Industry blogs and articles
  - Twitter/X threads from practitioners
  - Substack newsletters
  - Video talks and presentations
  - Open-source projects and frameworks
  - Company engineering blogs

Research topics:
  - Enterprise agent architectures and design patterns
  - Eval-first development and continuous evaluation
  - Agentic memory systems (short-term, long-term, structured)
  - Plugin/skill architectures for agents
  - RAG best practices for enterprise knowledge management
  - RAG vs. no-RAG decision framework: when is embedding-based RAG actually needed
    vs. fine-tuning, long-context prompting, or structured DB lookups
  - Optimal chunk sizing and article sizing strategies for KB (knowledge base)
    articles in RAG pipelines (semantic chunking, fixed-size, hierarchical, etc.)
  - When to use RAG: signal criteria (corpus size, update frequency, latency
    budget, factual grounding requirements, hallucination risk)
  - RAG quality improvement at scale: techniques to maintain/improve retrieval
    quality as the knowledge base grows larger (re-ranking, hybrid search,
    metadata filtering, query rewriting, HyDE, FLARE, self-RAG, corrective-RAG)
  - Multi-tenant agent systems
  - Auto-improving agents (prompt optimization, feedback loops)
  - Structured output and validation patterns
  - Tool/MCP ecosystems
  - Cost optimization without sacrificing quality
  - Observability, tracing, and debugging for agents
  - Plan-Act-Verify-Improve loops
  - Structured facet catalog retrieval: alternatives to dense vector search
    for finite, typed, metadata-rich knowledge bases
  - Two-stage and cascade retrieval: coarse filter + LLM rerank patterns
  - Ground-truth-as-few-shot: using historical labeled examples as dynamic
    RAG context to improve LLM output quality
  - Many-stage LLM pipeline collapse: when to merge stages vs. keep them
    separate, error accumulation in multi-step agentic chains
  - Typed tool / function-calling patterns for domain-specific retrieval
    (type-aware tools vs. one general semantic search)

TARGETED WEB RESEARCH KEYWORDS
  Search these specific subject areas explicitly during research. Each area
  should produce findings grounded in real implementations, papers, and blogs.

  Domain-specific enterprise agent research keywords:
    1. "enterprise marketing segmentation agent LLM 2024 2025"
    2. "enterprise CRM agent AI customer segmentation automation"
    3. "campaign agent architecture LLM marketing automation enterprise"
    4. "agentic CRM messaging personalization at scale AI"
    5. "customer data platform CDP AI agent integration architecture"
    6. "marketing campaign orchestration LLM agent enterprise"
    7. "audience segmentation AI agent structured output enterprise"
    8. "real-time customer segmentation LLM pipeline scalability"
    9. "enterprise AI agent facet retrieval structured catalog"
    10. "LLM-powered CRM segmentation Salesforce Adobe Walmart enterprise"
    11. "multi-tenant AI agent isolation tenant-specific knowledge retrieval"
    12. "enterprise agent pipeline stage collapse single LLM call quality"
    13. "RAG structured catalog finite knowledge base alternatives to embeddings"
    14. "hybrid BM25 dense retrieval short queries domain-specific benchmarks"
    15. "function calling typed tools LLM structured retrieval enterprise"
    16. "few-shot RAG ground truth historical examples dynamic context"
    17. "LLM reranker cross-encoder structured facet selection enterprise"
    18. "agentic segmentation campaign Salesforce Marketing Cloud AI"
    19. "Adobe Real-Time CDP AI segmentation agent architecture"
    20. "customer segmentation LLM Walmart Target Kroger retail enterprise"

  For each keyword area, find:
    - Concrete implementations or case studies (not just theory)
    - Architecture decisions made and why
    - Failure modes discovered and how they were addressed
    - Metrics: latency, accuracy, cost, scale
    - Open-source tools, frameworks, or repos if available


PART C: DELIVERABLE DOCUMENTS
--------------------------------------------------------------------------------
Produce all output in the research folder. Organize similar to the ditto_delivery
reference folder. Also create a new git project under the GitHub user
"snehalmistrybfa" so the research can be uploaded to the web for viewing detailed
research markdown files and analysis.

Required documents:

  DOCUMENT 1: BOTTLENECK ANALYSIS
  File: 01_bottleneck_analysis.md
  --------------------------------
  - Identify all bottlenecks and issues with the current Smart-Segmentation
    approach and code.
  - Cover: architecture issues, prompt design flaws, eval gaps, scalability
    limits, reliability problems, missing capabilities.
  - Dedicated section: "Facet Retrieval Bottlenecks" — analyze the Milvus
    embedding search for 500+ facets; identify where precision/recall fails
    using evidence from ground_truth_cdi(Updated Ground Truth).csv.
  - Dedicated section: "Pipeline Stage Overengineering or Gaps" — for each
    of the 7 pipeline stages (Route → Decompose → Date → Map → Classify →
    Format → Edit), identify whether the stage is necessary, over-engineered,
    or missing a verification step. Flag which stages accumulate errors.
  - Dedicated section: "Ground Truth Data Gap Analysis" — what do the
    expected vs. predicted facets reveal about systematic failure patterns?
    Which facet types (propensity, persona, purchase, date, numeric) fail most?
  - Dedicated section: "Multi-Tenant Scalability Risks" — what breaks first
    when catalog size doubles or a new tenant is onboarded?

  DOCUMENT 2: RESEARCH COMPENDIUM
  File: 02_research_compendium.md
  --------------------------------
  - All latest research, articles, blogs, tweets, substacks, videos, and papers
    explored during this investigation.
  - For each source: summary, what is useful, and how it applies to our
    enterprise agent goals.
  - Must be practical and actionable, grounded in real implementations.
  - Include state-of-the-art approaches with cost analysis.
  - Cost-saving alternatives should be justified but included in an appendix
    (end quality must remain at state-of-the-art level).
  - All citations and explanations from different companies, blogs, tweets,
    substacks, and papers must be included.

  DOCUMENT 3: CONCRETE UPGRADE PROPOSAL
  File: 03_concrete_upgrade_proposal.md
  --------------------------------
  - Detailed reformation and upgrade plan for Smart-Segmentation.
  - Start with high-level architecture, drill into details, then provide very
    specific transformation examples.
  - Include a concrete framework with detailed examples.
  - Identify bottlenecks at each level and propose solutions.
  - State-of-the-art approaches weighted against cost-based alternatives.
  - Cost-saving options justified in appendix; core quality must be best-in-class.
  - Dedicated section: "Facet Retrieval Upgrade" — concrete recommendation on
    whether to keep embedding search, replace it, augment it, or cascade it.
    Include decision matrix: embedding vs. structured lookup vs. hybrid vs.
    type-aware tools. Concrete before/after architecture diagram (ASCII or
    described precisely). Show how this scales to larger tenant catalogs.
  - Dedicated section: "Pipeline Stage Refactor" — propose the right number
    of stages. Show which stages merge, which stay separate, which become
    code (no LLM). Propose the static-prompt + dynamic-skill KB article
    pattern as a concrete alternative. Include example skill bundles for
    decompose, date-tag, map-facets, and format operations.
  - Dedicated section: "Ground Truth as Runtime Few-Shot RAG" — concrete
    proposal for using ground_truth_cdi CSV rows as dynamically retrieved
    few-shot examples at inference time (similarity search over historical
    segment definitions to find closest examples, inject as context).
  - Dedicated section: "Enterprise Domain Patterns" — what leading enterprise
    marketing/CRM/campaign AI agents do that this system should adopt
    (grounded in findings from targeted web research keyword areas).

  DOCUMENT 4: IMPLEMENTATION ROADMAP
  File: 04_implementation_roadmap.md
  --------------------------------
  - Step-by-step approach with clear priorities.
  - Most impactful work comes first.
  - Identify tasks that can be parallelized.
  - Explain hypothesis and reasoning behind each priority decision.
  - Include dependencies, risks, and mitigation strategies.

  DOCUMENT 0: INDEX
  File: INDEX.md
  --------------------------------
  - Overview of the research attempt, methodology, and links to all documents.


PART D: PRIOR RESEARCH CONTEXT (USE AS INPUT)
--------------------------------------------------------------------------------
The following criteria and architectural patterns were identified in earlier
research. Use these as a starting point and expand upon them.

1. CRITERIA FOR ENTERPRISE AGENT ADOPTION
   - Memory: retain context and information across interactions
   - Auto-improvement: learn from feedback and improve over time
   - Plugin architecture: add skills, knowledge, and tools dynamically
   - Reliable and consistent results

2. TECHNIQUES FOR AGENT RELIABILITY
   - Eval first, implement later
   - Use thinking models (structured reasoning)
   - Act -> Verify -> Improve loop
   - Auto-improving prompts (prompt optimization)

3. STATIC VS DYNAMIC PROMPT ARCHITECTURE
   What stays static (system + developer instructions):
     - Agent identity and goal (1-2 lines)
     - Safety and compliance rules
     - Output format requirements (schemas)
     - Control loop instruction (Plan -> Act -> Verify -> Improve)
     - Grounding rule: "Use tools + retrieved docs as source of truth; don't guess"

   What is dynamic (loaded at runtime):
     - Skills (workflows) loaded on demand
     - Knowledge (facts, policies, business rules) retrieved on demand
     - Tool inventory registered dynamically (plugins)
     - Tenant/client customizations stored as configs, not prompt text

4. SKILLS: ADD CAPABILITIES WITHOUT CHANGING THE PROMPT
   Skill = versioned, testable instruction bundle:
     - Name + description + triggers ("when to use")
     - Steps (deterministic procedure)
     - Input/output schema (structured)
     - Constraints + edge cases
     - Examples (few-shot) if needed

   Runtime flow:
     - Router picks skill(s) based on intent
     - Agent loads only the relevant skill text
     - Executes under the same static system prompt

   Implementation pattern:
     - Skill registry (DB/repo): {skill_id, version, metadata, instructions, evals}
     - Skill loader: inject selected skill into scratch context (not system prompt)
     - Eval gate: skill changes must pass evals before rollout (A/B per tenant)

5. KNOWLEDGE: ADD/EDIT CONTENT WITHOUT CHANGING THE PROMPT
   RAG + knowledge contracts:
     - Put knowledge in a retrievable store (docs, tables, policies, taxonomy)
     - Require citations/grounding ("answer only from retrieved sources when available")
     - Use chunking + metadata (domain, tenant, freshness, owner, version)
     - Add freshness rules ("do not use if older than X") for volatile facts

   Anti-hallucination rule:
     - If retrieval returns nothing relevant -> ask a question or say "unknown"
     - Never paraphrase prompts to fill gaps (this causes hallucinations)

6. PLUGINS/TOOLS: ADD TOOLS WITHOUT REWRITING PROMPTS
   Typed tools + tool registry:
     - Strong tool schemas (distinct names, strict args)
     - Operational descriptions (not vague)
     - Runtime tool enable/disable by tenant/environment
     - Policies: rate limits, auth scopes, allowlists/denylists, safe defaults

   Static prompt rules for tools:
     - Prompt does NOT list all tools (changes too often)
     - Instead: "Use available tools; select minimal tool needed; follow schemas exactly"
     - Tools are discovered from registry per request + policy

7. MODULAR AND LAYERED ARCHITECTURE

   A. Perception (input handling)
      - Normalize user input (intent, entities, tenant, permissions)
      - Route to skill(s) + retrieval queries

   B. Reasoning (planning and cognitive logic)
      - Use Plan -> Execute or ReAct loop
      - Keep consistent across all skills (static instruction)
      - Thinking models perform better with explicit, structured loops

   C. Memory (context management)
      - Short-term: conversation state, current task state
      - Long-term: user prefs, prior segment decisions, successful recipes
      - Store structured memories (not raw chat) and retrieve like knowledge

   D. Action (tool calls)
      - Tool selection: skill recipe + policy + confidence needs
      - Tool calls logged with inputs/outputs for debugging (traces)

   E. Feedback (self-assessment and improvement)
      - Post-run checks: schema validity, grounding coverage, policy checks
      - Automated evals: F1, task success, hallucination flags
      - Failures feed into: skill text + retrieval + tool defs evolve
        (prompt remains stable)

8. CONCRETE STATIC PROMPT TEMPLATE (WHAT STAYS FIXED)
   Operating rules (always follow):
     - Loop: Plan -> Act -> Verify -> Improve (with stop conditions)
     - Grounding: prefer tools/retrieval; don't invent
     - Formatting: return JSON with schema X (or strict sections)
     - Escalation: ask clarifying questions when missing required inputs
   Everything else (skills, knowledge, tenant policies, tools) loads dynamically.

9. RAG DECISION FRAMEWORK: WHEN AND HOW TO USE IT

   A. Do you actually need embedding-based RAG?
      Analyze and research each alternative before defaulting to RAG:
        - Fine-tuning: better when knowledge is static and patterns matter more
          than facts; not ideal for rapidly-changing KB articles
        - Long-context prompting (stuffing): viable when corpus is small (<100K
          tokens); evaluate cost vs. retrieval accuracy trade-off
        - Structured DB / SQL lookup: better for tabular, structured, or highly
          filterable data (customer records, product catalog, pricing tables)
        - Keyword / BM25 search: often underrated; combine with dense retrieval
          (hybrid search) for best results at lower cost
        - RAG is optimal when: corpus is large (>500 KB articles or frequently
          updated), answers require precise factual grounding, hallucination risk
          is high, and low latency is not a hard constraint

   B. Optimal chunk/article sizing for KB articles in RAG
      Research and provide concrete recommendations for:
        - Fixed-size chunking (token count): typical sweet spots (256, 512, 1024
          tokens) and when each applies
        - Semantic / paragraph-level chunking: when structure-aware splitting
          outperforms naive fixed-size splitting
        - Hierarchical chunking: large parent chunk for context + small child
          chunk for retrieval precision (parent-child retrieval pattern)
        - KB-article-level granularity: when to keep an entire article as one
          chunk vs. splitting; metadata tagging strategy per chunk
        - Overlap strategies: how much overlap between chunks prevents context
          loss at boundaries
        - Optimal embedding model selection relative to chunk size

   C. When to use RAG - decision criteria
      Provide a clear decision tree or rubric covering:
        - Corpus size threshold (how many articles/documents justify RAG?)
        - Update frequency (how often does knowledge change?)
        - Hallucination risk tolerance (safety-critical vs. low-stakes)
        - Latency budget (RAG adds retrieval latency; when is it acceptable?)
        - Query type fit (open-ended factual, procedural, comparative, etc.)
        - Cost model (embedding + retrieval + LLM vs. larger context window)

   D. Improving RAG quality as KB grows larger
      Research and document techniques to maintain and improve quality at scale:
        - Re-ranking: cross-encoder re-rankers (Cohere, BGE, etc.) applied post-
          retrieval to re-score top-K candidates
        - Hybrid search: BM25 + dense vector combined scoring (RRF fusion)
        - Query rewriting and expansion: HyDE (Hypothetical Document Embeddings),
          multi-query expansion, step-back prompting
        - Self-RAG and Corrective-RAG (CRAG): agent decides when to retrieve,
          verifies retrieved content quality, and retries if needed
        - FLARE (Forward-Looking Active Retrieval): retrieve proactively during
          generation when model confidence drops
        - Metadata filtering: tenant, domain, freshness, version filters to
          narrow the retrieval space before vector search
        - Knowledge graph augmentation: entity-linking + graph traversal for
          multi-hop reasoning on large KB
        - Eval-driven RAG tuning: measure retrieval recall, MRR, RAGAS metrics;
          use automated evals to detect quality degradation as KB grows
        - Indexing hygiene: deduplication, staleness detection, re-embedding
          triggers when articles are updated

10. KNOWN PROBLEMS TO SOLVE
   - Stop paraphrase-based prompt mutation
     -> Replace with skill selection + skill injection (versioned)
   - Make segment outputs strictly typed
     -> Structured outputs + validators + evals; reject invalid, retry with feedback
   - Tool + knowledge grounding for explainability
     -> Cite retrieved evidence (field metadata, model signals) instead of freeform
   - Feature work becomes "add a skill," not "rewrite the agent"
     -> Each epic becomes a skill pack with its own eval suite


INSTRUCTIONS FOR EXECUTION
--------------------------------------------------------------------------------
1. Read and deeply understand the Smart-Segmentation codebase first.
2. Conduct broad and deep research using web search, papers, and articles.
3. Cross-reference findings with the prior research context above.
4. Produce all four documents plus the INDEX.
5. Ensure all citations are included with URLs where available.
6. Be practical and actionable - this is for real implementation, not theory.
7. Place all output in:
   /Users/s0m0ohl/customer_segement/enterprise_agentic_research/research_1_sonnet_claude/


HTML VIEWER AUTO-HOOK
--------------------------------------------------------------------------------
An HTML viewer is already set up for this research. As you produce each document:

1. The sidebar in index.html will automatically detect available documents
   (green dot = available, gray dot = pending). No manual updates needed.

2. Each document you create (01_*.md through 04_*.md) will be instantly
   viewable through the HTML viewer at:
     research_1_sonnet_claude/index.html

3. The viewer renders markdown with an elegant dark/light theme, left sidebar
   navigation with breadcrumbs, and a card-based home screen.

4. After completing all documents, update INDEX.md to mark all deliverables
   as "Done" in the status table.

5. The hub page at enterprise_agentic_research/index.html links to all
   research attempts. If creating a new research attempt, add a card entry
   to the hub page as well.

Viewer URLs (open in browser):
  Hub:      enterprise_agentic_research/index.html
  Research: enterprise_agentic_research/research_1_sonnet_claude/index.html

GitHub:
  Repo:  https://github.com/snehalvinit/enterprise-agentic-research
  Pages: Enable GitHub Pages (Settings > Pages > main branch) to view online.


GIT WORKFLOW
--------------------------------------------------------------------------------
After producing documents, commit and push:
  cd /Users/s0m0ohl/customer_segement/enterprise_agentic_research
  git add -A
  git commit -m "Research 1: <brief description of what was added>"
  git push origin main


================================================================================
END OF PROMPT
================================================================================
